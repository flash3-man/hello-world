# æ•°æ®ä»“åº“è®¾è®¡æ–‡æ¡£

## ğŸ—ï¸ æ•°æ®ä»“åº“æ¶æ„æ¦‚è§ˆ

### æ•´ä½“æ¶æ„è®¾è®¡
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®æºå±‚ (Data Sources)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ ERPç³»ç»Ÿ     â”‚  â”‚ CRMç³»ç»Ÿ     â”‚  â”‚ ç”µå•†å¹³å°    â”‚        â”‚
â”‚  â”‚ (è®¢å•æ•°æ®)  â”‚  â”‚ (å®¢æˆ·æ•°æ®)  â”‚  â”‚ (é”€å”®æ•°æ®)  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ è´¢åŠ¡ç³»ç»Ÿ    â”‚  â”‚ åº“å­˜ç³»ç»Ÿ    â”‚  â”‚ ç‰©æµç³»ç»Ÿ    â”‚        â”‚
â”‚  â”‚ (è´¢åŠ¡æ•°æ®)  â”‚  â”‚ (åº“å­˜æ•°æ®)  â”‚  â”‚ (é…é€æ•°æ®)  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®é›†æˆå±‚ (ETL Layer)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ æ•°æ®æŠ½å–    â”‚  â”‚ æ•°æ®æ¸…æ´—    â”‚  â”‚ æ•°æ®åŠ è½½    â”‚        â”‚
â”‚  â”‚ (Extract)   â”‚  â”‚ (Transform) â”‚  â”‚ (Load)      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           Apache Airflow + Kafka + Spark                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®å­˜å‚¨å±‚ (Storage Layer)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ ODSå±‚       â”‚  â”‚ DWDå±‚       â”‚  â”‚ DWSå±‚       â”‚        â”‚
â”‚  â”‚ (åŸå§‹æ•°æ®)  â”‚  â”‚ (æ˜ç»†æ•°æ®)  â”‚  â”‚ (æ±‡æ€»æ•°æ®)  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ DIMå±‚       â”‚  â”‚ ADSå±‚       â”‚  â”‚ å®æ—¶æµ      â”‚        â”‚
â”‚  â”‚ (ç»´åº¦æ•°æ®)  â”‚  â”‚ (åº”ç”¨æ•°æ®)  â”‚  â”‚ (Stream)    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚              ClickHouse + Hive + Redis                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®æœåŠ¡å±‚ (Service Layer)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ OLAPå¼•æ“    â”‚  â”‚ æŸ¥è¯¢æœåŠ¡    â”‚  â”‚ APIç½‘å…³     â”‚        â”‚
â”‚  â”‚ (ClickHouse)â”‚  â”‚ (Presto)    â”‚  â”‚ (Gateway)   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åº”ç”¨å±‚ (Application Layer)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ ç§»åŠ¨ç«¯APP   â”‚  â”‚ Webç®¡ç†ç«¯   â”‚  â”‚ BIæŠ¥è¡¨      â”‚        â”‚
â”‚  â”‚ (Vue3)      â”‚  â”‚ (React)     â”‚  â”‚ (Tableau)   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š æ•°æ®ä»“åº“åˆ†å±‚è®¾è®¡

### 1. ODSå±‚ (Operational Data Store) - åŸå§‹æ•°æ®å±‚

#### è®¾è®¡åŸåˆ™
- ä¿æŒæ•°æ®åŸè²Œï¼Œä¸åšä»»ä½•å¤„ç†
- æŒ‰ä¸šåŠ¡ç³»ç»Ÿåˆ†åŒºå­˜å‚¨
- æ”¯æŒå…¨é‡å’Œå¢é‡åŒæ­¥
- æ•°æ®ä¿ç•™å‘¨æœŸï¼š3å¹´

#### è¡¨ç»“æ„è®¾è®¡
```sql
-- ERPè®¢å•åŸå§‹æ•°æ®è¡¨
CREATE TABLE ods_erp_orders (
    order_id STRING,
    customer_id STRING,
    employee_id STRING,
    order_date STRING,
    order_amount DECIMAL(15,2),
    order_status STRING,
    product_info STRING,
    create_time TIMESTAMP,
    update_time TIMESTAMP,
    etl_date STRING,
    source_system STRING
) PARTITIONED BY (dt STRING)
STORED AS PARQUET;

-- CRMå®¢æˆ·åŸå§‹æ•°æ®è¡¨
CREATE TABLE ods_crm_customers (
    customer_id STRING,
    customer_name STRING,
    customer_type STRING,
    contact_person STRING,
    phone STRING,
    address STRING,
    create_time TIMESTAMP,
    update_time TIMESTAMP,
    etl_date STRING,
    source_system STRING
) PARTITIONED BY (dt STRING)
STORED AS PARQUET;

-- åº“å­˜åŸå§‹æ•°æ®è¡¨
CREATE TABLE ods_inventory_products (
    product_id STRING,
    product_name STRING,
    category_id STRING,
    unit_price DECIMAL(10,2),
    stock_quantity INT,
    warehouse_id STRING,
    create_time TIMESTAMP,
    update_time TIMESTAMP,
    etl_date STRING,
    source_system STRING
) PARTITIONED BY (dt STRING)
STORED AS PARQUET;
```

### 2. DWDå±‚ (Data Warehouse Detail) - æ˜ç»†æ•°æ®å±‚

#### è®¾è®¡åŸåˆ™
- æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
- å»ºç«‹ç»Ÿä¸€çš„æ•°æ®æ¨¡å‹
- ä¿æŒä¸šåŠ¡è¿‡ç¨‹çš„åŸå­æ€§
- æ”¯æŒå†å²æ•°æ®è¿½æº¯

#### æ ¸å¿ƒäº‹å®è¡¨è®¾è®¡
```sql
-- è®¢å•æ˜ç»†äº‹å®è¡¨
CREATE TABLE dwd_order_detail_fact (
    order_detail_id STRING,
    order_id STRING,
    customer_id STRING,
    employee_id STRING,
    product_id STRING,
    order_date DATE,
    order_time TIMESTAMP,
    quantity INT,
    unit_price DECIMAL(10,2),
    discount_amount DECIMAL(10,2),
    total_amount DECIMAL(15,2),
    order_status STRING,
    payment_method STRING,
    delivery_address STRING,
    create_time TIMESTAMP,
    update_time TIMESTAMP,
    is_deleted TINYINT DEFAULT 0
) PARTITIONED BY (dt STRING)
STORED AS PARQUET;

-- é”€å”®æ˜ç»†äº‹å®è¡¨
CREATE TABLE dwd_sales_detail_fact (
    sales_id STRING,
    order_id STRING,
    customer_id STRING,
    employee_id STRING,
    product_id STRING,
    sales_date DATE,
    sales_time TIMESTAMP,
    sales_quantity INT,
    sales_amount DECIMAL(15,2),
    cost_amount DECIMAL(15,2),
    profit_amount DECIMAL(15,2),
    profit_rate DECIMAL(5,4),
    channel_id STRING,
    region_id STRING,
    create_time TIMESTAMP,
    update_time TIMESTAMP,
    is_deleted TINYINT DEFAULT 0
) PARTITIONED BY (dt STRING)
STORED AS PARQUET;

-- åº“å­˜å˜åŠ¨æ˜ç»†äº‹å®è¡¨
CREATE TABLE dwd_inventory_change_fact (
    change_id STRING,
    product_id STRING,
    warehouse_id STRING,
    change_type STRING, -- IN/OUT/ADJUST
    change_quantity INT,
    change_amount DECIMAL(15,2),
    before_quantity INT,
    after_quantity INT,
    change_date DATE,
    change_time TIMESTAMP,
    operator_id STRING,
    reason STRING,
    create_time TIMESTAMP,
    is_deleted TINYINT DEFAULT 0
) PARTITIONED BY (dt STRING)
STORED AS PARQUET;
```

### 3. DIMå±‚ (Dimension) - ç»´åº¦æ•°æ®å±‚

#### ç»´åº¦è¡¨è®¾è®¡
```sql
-- å‘˜å·¥ç»´åº¦è¡¨
CREATE TABLE dim_employee (
    employee_id STRING,
    employee_code STRING,
    employee_name STRING,
    department_id STRING,
    department_name STRING,
    position STRING,
    level STRING,
    manager_id STRING,
    hire_date DATE,
    status STRING,
    phone STRING,
    email STRING,
    region_id STRING,
    create_time TIMESTAMP,
    update_time TIMESTAMP,
    effective_date DATE,
    expiry_date DATE,
    is_current TINYINT DEFAULT 1
) STORED AS PARQUET;

-- å®¢æˆ·ç»´åº¦è¡¨
CREATE TABLE dim_customer (
    customer_id STRING,
    customer_code STRING,
    customer_name STRING,
    customer_type STRING, -- ä¸ªäºº/ä¼ä¸š/ä»£ç†å•†
    industry STRING,
    scale STRING, -- å¤§å‹/ä¸­å‹/å°å‹
    contact_person STRING,
    phone STRING,
    email STRING,
    address STRING,
    region_id STRING,
    credit_level STRING,
    create_time TIMESTAMP,
    update_time TIMESTAMP,
    effective_date DATE,
    expiry_date DATE,
    is_current TINYINT DEFAULT 1
) STORED AS PARQUET;

-- å•†å“ç»´åº¦è¡¨
CREATE TABLE dim_product (
    product_id STRING,
    product_code STRING,
    product_name STRING,
    category_id STRING,
    category_name STRING,
    brand STRING,
    specification STRING,
    unit STRING,
    unit_price DECIMAL(10,2),
    cost_price DECIMAL(10,2),
    status STRING,
    create_time TIMESTAMP,
    update_time TIMESTAMP,
    effective_date DATE,
    expiry_date DATE,
    is_current TINYINT DEFAULT 1
) STORED AS PARQUET;

-- æ—¶é—´ç»´åº¦è¡¨
CREATE TABLE dim_date (
    date_key STRING,
    full_date DATE,
    year INT,
    quarter INT,
    month INT,
    week INT,
    day INT,
    weekday INT,
    is_weekend TINYINT,
    is_holiday TINYINT,
    holiday_name STRING,
    fiscal_year INT,
    fiscal_quarter INT,
    fiscal_month INT
) STORED AS PARQUET;

-- åœ°åŒºç»´åº¦è¡¨
CREATE TABLE dim_region (
    region_id STRING,
    region_code STRING,
    region_name STRING,
    parent_region_id STRING,
    region_level INT, -- 1:çœ 2:å¸‚ 3:åŒºå¿
    region_path STRING,
    manager_id STRING,
    create_time TIMESTAMP,
    update_time TIMESTAMP,
    is_current TINYINT DEFAULT 1
) STORED AS PARQUET;
```

### 4. DWSå±‚ (Data Warehouse Summary) - æ±‡æ€»æ•°æ®å±‚

#### è½»åº¦æ±‡æ€»è¡¨è®¾è®¡
```sql
-- å‘˜å·¥æ—¥é”€å”®æ±‡æ€»è¡¨
CREATE TABLE dws_employee_sales_1d (
    employee_id STRING,
    stat_date DATE,
    order_count INT,
    sales_amount DECIMAL(15,2),
    profit_amount DECIMAL(15,2),
    profit_rate DECIMAL(5,4),
    customer_count INT,
    product_count INT,
    avg_order_amount DECIMAL(15,2),
    max_order_amount DECIMAL(15,2),
    return_count INT,
    return_amount DECIMAL(15,2),
    return_rate DECIMAL(5,4),
    create_time TIMESTAMP,
    update_time TIMESTAMP
) PARTITIONED BY (dt STRING)
STORED AS PARQUET;

-- å®¢æˆ·æ—¥æ¶ˆè´¹æ±‡æ€»è¡¨
CREATE TABLE dws_customer_consume_1d (
    customer_id STRING,
    stat_date DATE,
    order_count INT,
    consume_amount DECIMAL(15,2),
    product_count INT,
    avg_order_amount DECIMAL(15,2),
    max_order_amount DECIMAL(15,2),
    last_order_date DATE,
    create_time TIMESTAMP,
    update_time TIMESTAMP
) PARTITIONED BY (dt STRING)
STORED AS PARQUET;

-- å•†å“æ—¥é”€å”®æ±‡æ€»è¡¨
CREATE TABLE dws_product_sales_1d (
    product_id STRING,
    stat_date DATE,
    sales_quantity INT,
    sales_amount DECIMAL(15,2),
    profit_amount DECIMAL(15,2),
    profit_rate DECIMAL(5,4),
    order_count INT,
    customer_count INT,
    avg_price DECIMAL(10,2),
    inventory_quantity INT,
    create_time TIMESTAMP,
    update_time TIMESTAMP
) PARTITIONED BY (dt STRING)
STORED AS PARQUET;
```

### 5. ADSå±‚ (Application Data Service) - åº”ç”¨æ•°æ®å±‚

#### åº”ç”¨ä¸»é¢˜è¡¨è®¾è®¡
```sql
-- å‘˜å·¥ä¸šç»©åˆ†æè¡¨
CREATE TABLE ads_employee_performance (
    employee_id STRING,
    employee_name STRING,
    department_name STRING,
    stat_period STRING, -- daily/weekly/monthly/quarterly/yearly
    stat_date DATE,
    
    -- é”€å”®æŒ‡æ ‡
    sales_amount DECIMAL(15,2),
    sales_target DECIMAL(15,2),
    sales_completion_rate DECIMAL(5,4),
    sales_rank INT,
    
    -- è®¢å•æŒ‡æ ‡
    order_count INT,
    order_customers INT,
    order_products INT,
    avg_order_amount DECIMAL(15,2),
    
    -- å®¢æˆ·æŒ‡æ ‡
    visit_customers INT,
    new_customers INT,
    active_customers INT,
    customer_retention_rate DECIMAL(5,4),
    
    -- å•†å“æŒ‡æ ‡
    distribution_stores INT,
    distribution_products INT,
    inventory_turnover DECIMAL(5,2),
    
    create_time TIMESTAMP,
    update_time TIMESTAMP
) PARTITIONED BY (dt STRING, period STRING)
STORED AS PARQUET;

-- å®¢æˆ·ä»·å€¼åˆ†æè¡¨
CREATE TABLE ads_customer_value (
    customer_id STRING,
    customer_name STRING,
    customer_type STRING,
    stat_period STRING,
    stat_date DATE,
    
    -- æ¶ˆè´¹æŒ‡æ ‡
    total_amount DECIMAL(15,2),
    order_count INT,
    avg_order_amount DECIMAL(15,2),
    max_order_amount DECIMAL(15,2),
    
    -- è¡Œä¸ºæŒ‡æ ‡
    purchase_frequency DECIMAL(5,2),
    last_purchase_date DATE,
    purchase_cycle INT,
    
    -- ä»·å€¼åˆ†ç±»
    rfm_score STRING,
    customer_level STRING, -- é«˜ä»·å€¼/ä¸­ä»·å€¼/ä½ä»·å€¼
    lifecycle_stage STRING, -- æ–°å®¢æˆ·/æ´»è·ƒå®¢æˆ·/æ²‰ç¡å®¢æˆ·/æµå¤±å®¢æˆ·
    
    create_time TIMESTAMP,
    update_time TIMESTAMP
) PARTITIONED BY (dt STRING, period STRING)
STORED AS PARQUET;

-- å•†å“é”€å”®åˆ†æè¡¨
CREATE TABLE ads_product_analysis (
    product_id STRING,
    product_name STRING,
    category_name STRING,
    stat_period STRING,
    stat_date DATE,
    
    -- é”€å”®æŒ‡æ ‡
    sales_quantity INT,
    sales_amount DECIMAL(15,2),
    profit_amount DECIMAL(15,2),
    profit_rate DECIMAL(5,4),
    
    -- å¸‚åœºæŒ‡æ ‡
    market_share DECIMAL(5,4),
    sales_rank INT,
    growth_rate DECIMAL(5,4),
    
    -- åº“å­˜æŒ‡æ ‡
    inventory_quantity INT,
    inventory_amount DECIMAL(15,2),
    turnover_rate DECIMAL(5,2),
    stockout_days INT,
    
    -- åˆ†ç±»æ ‡ç­¾
    abc_category STRING, -- Aç±»/Bç±»/Cç±»å•†å“
    lifecycle_stage STRING, -- å¯¼å…¥æœŸ/æˆé•¿æœŸ/æˆç†ŸæœŸ/è¡°é€€æœŸ
    
    create_time TIMESTAMP,
    update_time TIMESTAMP
) PARTITIONED BY (dt STRING, period STRING)
STORED AS PARQUET;
```

## ğŸ”„ ETLæ•°æ®å¤„ç†æµç¨‹

### 1. æ•°æ®æŠ½å– (Extract)

#### å…¨é‡æŠ½å–ç­–ç•¥
```python
# å…¨é‡æ•°æ®æŠ½å–ç¤ºä¾‹
def extract_full_data(source_config, table_name, target_date):
    """
    å…¨é‡æ•°æ®æŠ½å–
    """
    sql = f"""
    SELECT *
    FROM {table_name}
    WHERE DATE(update_time) <= '{target_date}'
    """
    
    # æ‰§è¡ŒæŠ½å–
    data = execute_sql(source_config, sql)
    
    # æ·»åŠ ETLå…ƒæ•°æ®
    data['etl_date'] = target_date
    data['source_system'] = source_config['system_name']
    
    return data
```

#### å¢é‡æŠ½å–ç­–ç•¥
```python
# å¢é‡æ•°æ®æŠ½å–ç¤ºä¾‹
def extract_incremental_data(source_config, table_name, last_update_time):
    """
    å¢é‡æ•°æ®æŠ½å–
    """
    sql = f"""
    SELECT *
    FROM {table_name}
    WHERE update_time > '{last_update_time}'
    """
    
    # æ‰§è¡ŒæŠ½å–
    data = execute_sql(source_config, sql)
    
    # æ·»åŠ ETLå…ƒæ•°æ®
    data['etl_date'] = datetime.now().strftime('%Y-%m-%d')
    data['source_system'] = source_config['system_name']
    
    return data
```

### 2. æ•°æ®æ¸…æ´— (Transform)

#### æ•°æ®è´¨é‡è§„åˆ™
```python
# æ•°æ®æ¸…æ´—è§„åˆ™ç¤ºä¾‹
def clean_order_data(raw_data):
    """
    è®¢å•æ•°æ®æ¸…æ´—
    """
    cleaned_data = raw_data.copy()
    
    # 1. ç©ºå€¼å¤„ç†
    cleaned_data['customer_name'] = cleaned_data['customer_name'].fillna('æœªçŸ¥å®¢æˆ·')
    cleaned_data['order_amount'] = cleaned_data['order_amount'].fillna(0)
    
    # 2. æ•°æ®ç±»å‹è½¬æ¢
    cleaned_data['order_date'] = pd.to_datetime(cleaned_data['order_date'])
    cleaned_data['order_amount'] = cleaned_data['order_amount'].astype(float)
    
    # 3. æ•°æ®æ ¡éªŒ
    # è®¢å•é‡‘é¢ä¸èƒ½ä¸ºè´Ÿæ•°
    cleaned_data = cleaned_data[cleaned_data['order_amount'] >= 0]
    
    # è®¢å•æ—¥æœŸä¸èƒ½è¶…è¿‡å½“å‰æ—¥æœŸ
    cleaned_data = cleaned_data[cleaned_data['order_date'] <= datetime.now()]
    
    # 4. æ•°æ®æ ‡å‡†åŒ–
    # å®¢æˆ·ç±»å‹æ ‡å‡†åŒ–
    type_mapping = {
        'ä¸ªäºº': 'INDIVIDUAL',
        'ä¼ä¸š': 'ENTERPRISE',
        'ä»£ç†å•†': 'AGENT'
    }
    cleaned_data['customer_type'] = cleaned_data['customer_type'].map(type_mapping)
    
    # 5. æ•°æ®ä¸°å¯Œ
    # æ·»åŠ è®¢å•å¹´æœˆ
    cleaned_data['order_year'] = cleaned_data['order_date'].dt.year
    cleaned_data['order_month'] = cleaned_data['order_date'].dt.month
    
    return cleaned_data
```

### 3. æ•°æ®åŠ è½½ (Load)

#### æ‰¹é‡åŠ è½½ç­–ç•¥
```python
# æ‰¹é‡æ•°æ®åŠ è½½ç¤ºä¾‹
def load_data_to_warehouse(data, target_table, partition_date):
    """
    æ•°æ®åŠ è½½åˆ°æ•°æ®ä»“åº“
    """
    # 1. æ•°æ®åˆ†åŒº
    partitioned_data = data.copy()
    partitioned_data['dt'] = partition_date
    
    # 2. æ•°æ®å»é‡
    partitioned_data = partitioned_data.drop_duplicates(
        subset=['order_id'], keep='last'
    )
    
    # 3. å†™å…¥ClickHouse
    clickhouse_client.insert_dataframe(
        f"INSERT INTO {target_table}", 
        partitioned_data
    )
    
    # 4. æ›´æ–°å…ƒæ•°æ®
    update_metadata(target_table, partition_date, len(partitioned_data))
```

## ğŸ“ˆ å®æ—¶æ•°æ®å¤„ç†

### 1. æµå¼æ•°æ®æ¶æ„
```yaml
# Kafkaé…ç½®
kafka_config:
  bootstrap_servers: "kafka1:9092,kafka2:9092,kafka3:9092"
  topics:
    - name: "order_events"
      partitions: 12
      replication_factor: 3
    - name: "sales_events"
      partitions: 12
      replication_factor: 3
    - name: "inventory_events"
      partitions: 6
      replication_factor: 3

# Flinké…ç½®
flink_config:
  parallelism: 12
  checkpoint_interval: 60000
  state_backend: "rocksdb"
  
# ClickHouseé…ç½®
clickhouse_config:
  cluster: "business_analysis_cluster"
  database: "real_time_dw"
  buffer_size: 1000000
  flush_interval: 10000
```

### 2. å®æ—¶æŒ‡æ ‡è®¡ç®—
```sql
-- å®æ—¶é”€å”®æŒ‡æ ‡è¡¨
CREATE TABLE rt_sales_metrics (
    employee_id String,
    stat_time DateTime,
    sales_amount_1h AggregateFunction(sum, Decimal64(2)),
    sales_count_1h AggregateFunction(count, UInt64),
    sales_amount_1d AggregateFunction(sum, Decimal64(2)),
    sales_count_1d AggregateFunction(count, UInt64),
    update_time DateTime DEFAULT now()
) ENGINE = AggregatingMergeTree()
PARTITION BY toYYYYMMDD(stat_time)
ORDER BY (employee_id, stat_time)
TTL stat_time + INTERVAL 7 DAY;

-- å®æ—¶æŒ‡æ ‡ç‰©åŒ–è§†å›¾
CREATE MATERIALIZED VIEW rt_sales_metrics_mv TO rt_sales_metrics AS
SELECT
    employee_id,
    toStartOfHour(event_time) as stat_time,
    sumState(sales_amount) as sales_amount_1h,
    countState() as sales_count_1h,
    sumState(sales_amount) as sales_amount_1d,
    countState() as sales_count_1d
FROM kafka_sales_events
GROUP BY employee_id, stat_time;
```

## ğŸ¯ æ•°æ®å»ºæ¨¡æœ€ä½³å®è·µ

### 1. æ˜Ÿå‹æ¨¡å‹è®¾è®¡

#### é”€å”®ä¸»é¢˜æ˜Ÿå‹æ¨¡å‹
```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   dim_date      â”‚
                    â”‚  (æ—¶é—´ç»´åº¦)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  dim_employee   â”‚      â”‚      â”‚  dim_customer   â”‚
    â”‚   (å‘˜å·¥ç»´åº¦)    â”‚      â”‚      â”‚   (å®¢æˆ·ç»´åº¦)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚              â”‚              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ fact_sales      â”‚
                    â”‚  (é”€å”®äº‹å®è¡¨)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚              â”‚              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  dim_product    â”‚      â”‚      â”‚  dim_region     â”‚
    â”‚   (å•†å“ç»´åº¦)    â”‚      â”‚      â”‚   (åœ°åŒºç»´åº¦)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  dim_channel    â”‚
                    â”‚   (æ¸ é“ç»´åº¦)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. æ•°æ®è´¨é‡ç®¡ç†

#### æ•°æ®è´¨é‡æ£€æŸ¥è§„åˆ™
```python
# æ•°æ®è´¨é‡æ£€æŸ¥æ¡†æ¶
class DataQualityChecker:
    def __init__(self):
        self.rules = []

    def add_rule(self, rule):
        self.rules.append(rule)

    def check_completeness(self, df, required_columns):
        """å®Œæ•´æ€§æ£€æŸ¥"""
        results = []
        for col in required_columns:
            null_count = df[col].isnull().sum()
            null_rate = null_count / len(df)
            results.append({
                'column': col,
                'null_count': null_count,
                'null_rate': null_rate,
                'status': 'PASS' if null_rate < 0.05 else 'FAIL'
            })
        return results

    def check_accuracy(self, df, column, valid_range):
        """å‡†ç¡®æ€§æ£€æŸ¥"""
        min_val, max_val = valid_range
        invalid_count = len(df[(df[column] < min_val) | (df[column] > max_val)])
        invalid_rate = invalid_count / len(df)

        return {
            'column': column,
            'invalid_count': invalid_count,
            'invalid_rate': invalid_rate,
            'status': 'PASS' if invalid_rate < 0.01 else 'FAIL'
        }

    def check_consistency(self, df, rule_func):
        """ä¸€è‡´æ€§æ£€æŸ¥"""
        violations = df[~df.apply(rule_func, axis=1)]
        violation_rate = len(violations) / len(df)

        return {
            'violation_count': len(violations),
            'violation_rate': violation_rate,
            'status': 'PASS' if violation_rate < 0.01 else 'FAIL'
        }

# ä½¿ç”¨ç¤ºä¾‹
checker = DataQualityChecker()

# æ£€æŸ¥è®¢å•æ•°æ®è´¨é‡
order_quality = checker.check_completeness(
    order_df,
    ['order_id', 'customer_id', 'order_amount']
)

amount_quality = checker.check_accuracy(
    order_df,
    'order_amount',
    (0, 1000000)
)
```

### 3. æ•°æ®è¡€ç¼˜ç®¡ç†

#### è¡€ç¼˜å…³ç³»å®šä¹‰
```yaml
# æ•°æ®è¡€ç¼˜é…ç½®
data_lineage:
  tables:
    - name: "ads_employee_performance"
      type: "ADS"
      dependencies:
        - table: "dws_employee_sales_1d"
          type: "DWS"
          columns:
            - source: "sales_amount"
              target: "sales_amount"
            - source: "order_count"
              target: "order_count"
        - table: "dim_employee"
          type: "DIM"
          columns:
            - source: "employee_name"
              target: "employee_name"
            - source: "department_name"
              target: "department_name"

    - name: "dws_employee_sales_1d"
      type: "DWS"
      dependencies:
        - table: "dwd_sales_detail_fact"
          type: "DWD"
          transformation: "GROUP BY employee_id, stat_date"

    - name: "dwd_sales_detail_fact"
      type: "DWD"
      dependencies:
        - table: "ods_erp_orders"
          type: "ODS"
          transformation: "JOIN with dim_product, dim_customer"
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. åˆ†åŒºç­–ç•¥

#### æ—¶é—´åˆ†åŒº
```sql
-- æŒ‰æ—¥æœŸåˆ†åŒºçš„å¤§è¡¨
CREATE TABLE fact_sales_detail (
    sale_id String,
    order_id String,
    customer_id String,
    employee_id String,
    product_id String,
    sale_date Date,
    sale_amount Decimal64(2),
    quantity UInt32,
    create_time DateTime
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(sale_date)  -- æŒ‰æœˆåˆ†åŒº
ORDER BY (customer_id, employee_id, sale_date)
TTL sale_date + INTERVAL 3 YEAR;  -- 3å¹´åè‡ªåŠ¨åˆ é™¤

-- æŒ‰ä¸šåŠ¡ç»´åº¦åˆ†åŒº
CREATE TABLE fact_inventory_change (
    change_id String,
    product_id String,
    warehouse_id String,
    change_type Enum8('IN'=1, 'OUT'=2, 'ADJUST'=3),
    change_quantity Int32,
    change_date Date,
    create_time DateTime
) ENGINE = MergeTree()
PARTITION BY (toYYYYMM(change_date), change_type)  -- å¤åˆåˆ†åŒº
ORDER BY (product_id, warehouse_id, change_date);
```

### 2. ç´¢å¼•ä¼˜åŒ–

#### ä¸»é”®ç´¢å¼•è®¾è®¡
```sql
-- é«˜æ•ˆçš„ä¸»é”®ç´¢å¼•è®¾è®¡
CREATE TABLE dws_customer_behavior (
    customer_id String,
    behavior_date Date,
    page_views UInt32,
    session_duration UInt32,
    purchase_amount Decimal64(2),
    create_time DateTime
) ENGINE = MergeTree()
ORDER BY (customer_id, behavior_date)  -- æŒ‰æŸ¥è¯¢æ¨¡å¼è®¾è®¡ä¸»é”®
SETTINGS index_granularity = 8192;

-- è·³æ•°ç´¢å¼•
ALTER TABLE dws_customer_behavior
ADD INDEX idx_purchase_amount purchase_amount TYPE minmax GRANULARITY 4;

-- å¸ƒéš†è¿‡æ»¤å™¨ç´¢å¼•
ALTER TABLE dws_customer_behavior
ADD INDEX idx_customer_id customer_id TYPE bloom_filter GRANULARITY 1;
```

### 3. ç‰©åŒ–è§†å›¾ä¼˜åŒ–

#### é¢„èšåˆè§†å›¾
```sql
-- å‘˜å·¥é”€å”®æ±‡æ€»ç‰©åŒ–è§†å›¾
CREATE MATERIALIZED VIEW mv_employee_sales_summary
ENGINE = SummingMergeTree()
ORDER BY (employee_id, toYYYYMM(sale_date))
AS SELECT
    employee_id,
    toYYYYMM(sale_date) as sale_month,
    sum(sale_amount) as total_sales,
    count() as order_count,
    uniq(customer_id) as customer_count
FROM fact_sales_detail
GROUP BY employee_id, sale_month;

-- å®æ—¶æŒ‡æ ‡ç‰©åŒ–è§†å›¾
CREATE MATERIALIZED VIEW mv_realtime_metrics
ENGINE = AggregatingMergeTree()
ORDER BY (metric_type, toStartOfMinute(event_time))
AS SELECT
    'sales' as metric_type,
    toStartOfMinute(event_time) as time_window,
    sumState(amount) as total_amount,
    countState() as event_count,
    uniqState(customer_id) as unique_customers
FROM kafka_events
WHERE event_type = 'sale'
GROUP BY metric_type, time_window;
```

## ğŸ“Š OLAPæŸ¥è¯¢ä¼˜åŒ–

### 1. å¤šç»´åˆ†ææŸ¥è¯¢

#### é’»å–æŸ¥è¯¢ç¤ºä¾‹
```sql
-- é”€å”®æ•°æ®é’»å–ï¼šå¹´ -> å­£åº¦ -> æœˆ -> æ—¥
-- å¹´åº¦æ±‡æ€»
SELECT
    toYear(sale_date) as year,
    sum(sale_amount) as total_sales,
    count() as order_count,
    uniq(customer_id) as customer_count
FROM fact_sales_detail
WHERE sale_date >= '2024-01-01'
GROUP BY year
ORDER BY year;

-- é’»å–åˆ°å­£åº¦
SELECT
    toYear(sale_date) as year,
    toQuarter(sale_date) as quarter,
    sum(sale_amount) as total_sales,
    count() as order_count
FROM fact_sales_detail
WHERE toYear(sale_date) = 2024
GROUP BY year, quarter
ORDER BY year, quarter;

-- é’»å–åˆ°æœˆä»½
SELECT
    toYear(sale_date) as year,
    toMonth(sale_date) as month,
    sum(sale_amount) as total_sales,
    count() as order_count
FROM fact_sales_detail
WHERE toYear(sale_date) = 2024
  AND toQuarter(sale_date) = 1
GROUP BY year, month
ORDER BY year, month;
```

#### åˆ‡ç‰‡åˆ‡å—æŸ¥è¯¢
```sql
-- æŒ‰åœ°åŒºåˆ‡ç‰‡
SELECT
    r.region_name,
    sum(f.sale_amount) as total_sales,
    avg(f.sale_amount) as avg_sales
FROM fact_sales_detail f
JOIN dim_region r ON f.region_id = r.region_id
WHERE f.sale_date BETWEEN '2024-01-01' AND '2024-12-31'
GROUP BY r.region_name
ORDER BY total_sales DESC;

-- æŒ‰å•†å“ç±»åˆ«åˆ‡å—
SELECT
    p.category_name,
    toMonth(f.sale_date) as month,
    sum(f.sale_amount) as monthly_sales
FROM fact_sales_detail f
JOIN dim_product p ON f.product_id = p.product_id
WHERE f.sale_date >= '2024-01-01'
  AND p.category_name IN ('é¥®æ–™', 'é£Ÿå“', 'æ—¥ç”¨å“')
GROUP BY p.category_name, month
ORDER BY category_name, month;
```

### 2. å¤æ‚åˆ†ææŸ¥è¯¢

#### åŒæ¯”ç¯æ¯”åˆ†æ
```sql
-- é”€å”®åŒæ¯”ç¯æ¯”åˆ†æ
WITH monthly_sales AS (
    SELECT
        toYYYYMM(sale_date) as month_key,
        toYear(sale_date) as year,
        toMonth(sale_date) as month,
        sum(sale_amount) as monthly_amount
    FROM fact_sales_detail
    WHERE sale_date >= '2023-01-01'
    GROUP BY month_key, year, month
)
SELECT
    month_key,
    year,
    month,
    monthly_amount,
    -- ç¯æ¯”å¢é•¿ç‡
    (monthly_amount - lag(monthly_amount, 1) OVER (ORDER BY month_key)) /
    lag(monthly_amount, 1) OVER (ORDER BY month_key) * 100 as mom_growth_rate,
    -- åŒæ¯”å¢é•¿ç‡
    (monthly_amount - lag(monthly_amount, 12) OVER (ORDER BY month_key)) /
    lag(monthly_amount, 12) OVER (ORDER BY month_key) * 100 as yoy_growth_rate
FROM monthly_sales
ORDER BY month_key;
```

#### RFMå®¢æˆ·åˆ†æ
```sql
-- RFMå®¢æˆ·ä»·å€¼åˆ†æ
WITH customer_rfm AS (
    SELECT
        customer_id,
        -- Recency: æœ€è¿‘è´­ä¹°è·ä»Šå¤©æ•°
        dateDiff('day', max(sale_date), today()) as recency,
        -- Frequency: è´­ä¹°é¢‘æ¬¡
        count() as frequency,
        -- Monetary: è´­ä¹°é‡‘é¢
        sum(sale_amount) as monetary
    FROM fact_sales_detail
    WHERE sale_date >= today() - INTERVAL 1 YEAR
    GROUP BY customer_id
),
rfm_scores AS (
    SELECT
        customer_id,
        recency,
        frequency,
        monetary,
        -- RFMè¯„åˆ† (1-5åˆ†)
        CASE
            WHEN recency <= 30 THEN 5
            WHEN recency <= 60 THEN 4
            WHEN recency <= 90 THEN 3
            WHEN recency <= 180 THEN 2
            ELSE 1
        END as r_score,

        CASE
            WHEN frequency >= 20 THEN 5
            WHEN frequency >= 10 THEN 4
            WHEN frequency >= 5 THEN 3
            WHEN frequency >= 2 THEN 2
            ELSE 1
        END as f_score,

        CASE
            WHEN monetary >= 50000 THEN 5
            WHEN monetary >= 20000 THEN 4
            WHEN monetary >= 10000 THEN 3
            WHEN monetary >= 5000 THEN 2
            ELSE 1
        END as m_score
    FROM customer_rfm
)
SELECT
    customer_id,
    r_score,
    f_score,
    m_score,
    concat(toString(r_score), toString(f_score), toString(m_score)) as rfm_code,
    CASE
        WHEN r_score >= 4 AND f_score >= 4 AND m_score >= 4 THEN 'é‡è¦ä»·å€¼å®¢æˆ·'
        WHEN r_score >= 4 AND f_score >= 3 AND m_score >= 3 THEN 'é‡è¦å‘å±•å®¢æˆ·'
        WHEN r_score >= 3 AND f_score >= 4 AND m_score >= 4 THEN 'é‡è¦ä¿æŒå®¢æˆ·'
        WHEN r_score >= 3 AND f_score >= 3 AND m_score >= 3 THEN 'é‡è¦æŒ½ç•™å®¢æˆ·'
        WHEN r_score <= 2 AND f_score >= 3 AND m_score >= 3 THEN 'ä¸€èˆ¬ä»·å€¼å®¢æˆ·'
        ELSE 'ä¸€èˆ¬å®¢æˆ·'
    END as customer_segment
FROM rfm_scores
ORDER BY r_score DESC, f_score DESC, m_score DESC;
```

## ğŸ”§ æ•°æ®æ²»ç†æ¡†æ¶

### 1. å…ƒæ•°æ®ç®¡ç†

#### å…ƒæ•°æ®æ¨¡å‹
```sql
-- è¡¨å…ƒæ•°æ®
CREATE TABLE metadata_tables (
    table_id String,
    database_name String,
    table_name String,
    table_type Enum8('ODS'=1, 'DWD'=2, 'DWS'=3, 'DIM'=4, 'ADS'=5),
    description String,
    owner String,
    create_time DateTime,
    update_time DateTime,
    row_count UInt64,
    data_size UInt64,
    partition_key String,
    sort_key String,
    ttl_expression String
) ENGINE = MergeTree()
ORDER BY (database_name, table_name);

-- å­—æ®µå…ƒæ•°æ®
CREATE TABLE metadata_columns (
    column_id String,
    table_id String,
    column_name String,
    data_type String,
    is_nullable UInt8,
    default_value String,
    description String,
    business_rules String,
    data_quality_rules String,
    create_time DateTime,
    update_time DateTime
) ENGINE = MergeTree()
ORDER BY (table_id, column_name);

-- æ•°æ®è¡€ç¼˜å…³ç³»
CREATE TABLE metadata_lineage (
    lineage_id String,
    source_table_id String,
    target_table_id String,
    source_column String,
    target_column String,
    transformation_logic String,
    dependency_type Enum8('DIRECT'=1, 'INDIRECT'=2),
    create_time DateTime,
    update_time DateTime
) ENGINE = MergeTree()
ORDER BY (source_table_id, target_table_id);
```

### 2. æ•°æ®å®‰å…¨ç®¡ç†

#### æƒé™æ§åˆ¶ç­–ç•¥
```sql
-- åˆ›å»ºè§’è‰²
CREATE ROLE data_analyst;
CREATE ROLE data_engineer;
CREATE ROLE business_user;

-- åˆ†é…æƒé™
-- æ•°æ®åˆ†æå¸ˆï¼šå¯ä»¥æŸ¥è¯¢æ‰€æœ‰DWSå’ŒADSå±‚æ•°æ®
GRANT SELECT ON business_dw.dws_* TO data_analyst;
GRANT SELECT ON business_dw.ads_* TO data_analyst;

-- æ•°æ®å·¥ç¨‹å¸ˆï¼šå¯ä»¥æ“ä½œæ‰€æœ‰å±‚çº§æ•°æ®
GRANT ALL ON business_dw.* TO data_engineer;

-- ä¸šåŠ¡ç”¨æˆ·ï¼šåªèƒ½æŸ¥è¯¢ADSå±‚æ•°æ®
GRANT SELECT ON business_dw.ads_* TO business_user;

-- è¡Œçº§å®‰å…¨æ§åˆ¶
CREATE ROW POLICY employee_data_policy ON business_dw.ads_employee_performance
FOR SELECT USING employee_id IN (
    SELECT employee_id
    FROM dim_employee
    WHERE manager_id = currentUser()
) TO business_user;
```

### 3. æ•°æ®ç›‘æ§å‘Šè­¦

#### ç›‘æ§æŒ‡æ ‡å®šä¹‰
```python
# æ•°æ®ç›‘æ§é…ç½®
monitoring_config = {
    "data_freshness": {
        "tables": ["dws_employee_sales_1d", "dws_customer_consume_1d"],
        "max_delay_hours": 2,
        "alert_channels": ["email", "slack"]
    },

    "data_quality": {
        "completeness_threshold": 0.95,
        "accuracy_threshold": 0.99,
        "consistency_threshold": 0.98,
        "alert_channels": ["email", "dingding"]
    },

    "system_performance": {
        "query_timeout_seconds": 300,
        "cpu_usage_threshold": 80,
        "memory_usage_threshold": 85,
        "disk_usage_threshold": 90,
        "alert_channels": ["email", "sms"]
    }
}

# ç›‘æ§ä»»åŠ¡å®ç°
class DataMonitor:
    def check_data_freshness(self, table_name, max_delay_hours):
        """æ£€æŸ¥æ•°æ®æ–°é²œåº¦"""
        sql = f"""
        SELECT
            max(dt) as latest_partition,
            dateDiff('hour', toDateTime(max(dt)), now()) as delay_hours
        FROM {table_name}
        """
        result = self.execute_query(sql)

        if result['delay_hours'] > max_delay_hours:
            self.send_alert(
                f"æ•°æ®å»¶è¿Ÿå‘Šè­¦: {table_name} æ•°æ®å»¶è¿Ÿ {result['delay_hours']} å°æ—¶",
                "data_freshness"
            )

    def check_data_volume(self, table_name, expected_range):
        """æ£€æŸ¥æ•°æ®é‡å¼‚å¸¸"""
        sql = f"""
        SELECT count() as row_count
        FROM {table_name}
        WHERE dt = yesterday()
        """
        result = self.execute_query(sql)

        min_count, max_count = expected_range
        if not (min_count <= result['row_count'] <= max_count):
            self.send_alert(
                f"æ•°æ®é‡å¼‚å¸¸: {table_name} æ˜¨æ—¥æ•°æ®é‡ {result['row_count']}",
                "data_volume"
            )
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2024å¹´8æœˆ14æ—¥
**ç»´æŠ¤å›¢é˜Ÿ**: æ•°æ®å›¢é˜Ÿ

> ğŸ“Š **é‡è¦è¯´æ˜**: æœ¬æ–‡æ¡£æä¾›äº†å®Œæ•´çš„æ•°æ®ä»“åº“è®¾è®¡æ–¹æ¡ˆï¼ŒåŒ…æ‹¬åˆ†å±‚æ¶æ„ã€å»ºæ¨¡è§„èŒƒã€ETLæµç¨‹ã€æ€§èƒ½ä¼˜åŒ–å’Œæ•°æ®æ²»ç†ç­‰æ–¹é¢ã€‚åœ¨å®é™…å®æ–½æ—¶ï¼Œè¯·æ ¹æ®å…·ä½“ä¸šåŠ¡éœ€æ±‚å’ŒæŠ€æœ¯ç¯å¢ƒè¿›è¡Œè°ƒæ•´ã€‚
```
