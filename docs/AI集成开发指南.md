# AIé›†æˆå¼€å‘æŒ‡å—

## ğŸ¤– AIé©±åŠ¨å¼€å‘æ¨¡å¼

### AIåœ¨é¡¹ç›®ä¸­çš„åº”ç”¨åœºæ™¯
1. **éœ€æ±‚åˆ†æ**: AIè¾…åŠ©éœ€æ±‚ç†è§£å’ŒåŠŸèƒ½è§„æ ¼ç”Ÿæˆ
2. **ä»£ç ç”Ÿæˆ**: AIç”Ÿæˆæ ‡å‡†åŒ–çš„CRUDä»£ç å’Œä¸šåŠ¡é€»è¾‘
3. **æµ‹è¯•ç”Ÿæˆ**: AIç”Ÿæˆå•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•ç”¨ä¾‹
4. **æ–‡æ¡£ç”Ÿæˆ**: AIè‡ªåŠ¨ç”ŸæˆæŠ€æœ¯æ–‡æ¡£å’ŒAPIæ–‡æ¡£
5. **ä»£ç å®¡æŸ¥**: AIè¾…åŠ©ä»£ç è´¨é‡æ£€æŸ¥å’Œä¼˜åŒ–å»ºè®®
6. **ä¸šåŠ¡åˆ†æ**: AIåˆ†æä¸šåŠ¡æ•°æ®ï¼Œæä¾›æ™ºèƒ½æ´å¯Ÿ

### AIå¼€å‘å·¥ä½œæµ
```
éœ€æ±‚è¾“å…¥ â†’ AIåˆ†æ â†’ ä»£ç ç”Ÿæˆ â†’ äººå·¥å®¡æŸ¥ â†’ æµ‹è¯•éªŒè¯ â†’ éƒ¨ç½²ä¸Šçº¿
    â†“        â†“        â†“        â†“        â†“        â†“
ä¸šåŠ¡æè¿°   åŠŸèƒ½æ‹†è§£   æ ‡å‡†ä»£ç    è´¨é‡æ£€æŸ¥   è‡ªåŠ¨æµ‹è¯•   æŒç»­ç›‘æ§
```

## ğŸ§  AIä»£ç ç”Ÿæˆè§„èŒƒ

### 1. Controllerä»£ç ç”Ÿæˆæ¨¡æ¿
```yaml
# AIä»£ç ç”Ÿæˆé…ç½®
ai_code_generation:
  controller_template:
    input: |
      ä¸šåŠ¡å®ä½“: å‘˜å·¥ä¸šç»© (EmployeePerformance)
      ä¸»è¦å­—æ®µ: employeeId, employeeName, salesAmount, orderCount
      ä¸šåŠ¡æ“ä½œ: æŸ¥è¯¢åˆ—è¡¨, æŸ¥è¯¢è¯¦æƒ…, å¯¼å‡ºæ•°æ®
      æƒé™æ§åˆ¶: performance:employee:list, performance:employee:detail
    
    output_pattern: |
      @RestController
      @RequestMapping("/api/v1/{resource}")
      @Api(tags = "{resource_name}ç®¡ç†")
      public class {EntityName}Controller {
          
          @Autowired
          private I{EntityName}Service {entityName}Service;
          
          @GetMapping("/list")
          @PreAuthorize("hasAuthority('{resource}:list')")
          public Result<PageResponse<{EntityName}DTO>> get{EntityName}List(@Valid {EntityName}QueryDTO queryDTO) {
              // AIç”Ÿæˆçš„æ ‡å‡†æŸ¥è¯¢é€»è¾‘
          }
          
          @GetMapping("/{id}")
          @PreAuthorize("hasAuthority('{resource}:detail')")
          public Result<{EntityName}DTO> get{EntityName}Detail(@PathVariable Long id) {
              // AIç”Ÿæˆçš„è¯¦æƒ…æŸ¥è¯¢é€»è¾‘
          }
      }
```

### 2. Serviceä»£ç ç”Ÿæˆæ¨¡æ¿
```java
/**
 * AIç”Ÿæˆçš„Serviceæ¥å£æ¨¡æ¿
 */
public interface I{EntityName}Service {
    
    /**
     * åˆ†é¡µæŸ¥è¯¢{entityDescription}
     * 
     * @param queryDTO æŸ¥è¯¢æ¡ä»¶
     * @return åˆ†é¡µç»“æœ
     */
    PageResponse<{EntityName}DTO> get{EntityName}List({EntityName}QueryDTO queryDTO);
    
    /**
     * æ ¹æ®IDæŸ¥è¯¢{entityDescription}è¯¦æƒ…
     * 
     * @param id ä¸»é”®ID
     * @return {entityDescription}è¯¦æƒ…
     */
    {EntityName}DTO get{EntityName}Detail(Long id);
    
    /**
     * æ–°å¢{entityDescription}
     * 
     * @param createDTO åˆ›å»ºå‚æ•°
     * @return åˆ›å»ºç»“æœ
     */
    Long create{EntityName}({EntityName}CreateDTO createDTO);
    
    /**
     * æ›´æ–°{entityDescription}
     * 
     * @param id ä¸»é”®ID
     * @param updateDTO æ›´æ–°å‚æ•°
     * @return æ›´æ–°ç»“æœ
     */
    Boolean update{EntityName}(Long id, {EntityName}UpdateDTO updateDTO);
    
    /**
     * åˆ é™¤{entityDescription}
     * 
     * @param id ä¸»é”®ID
     * @return åˆ é™¤ç»“æœ
     */
    Boolean delete{EntityName}(Long id);
}

/**
 * AIç”Ÿæˆçš„Serviceå®ç°æ¨¡æ¿
 */
@Service
@Slf4j
@Transactional(rollbackFor = Exception.class)
public class {EntityName}ServiceImpl implements I{EntityName}Service {
    
    @Autowired
    private {EntityName}Mapper {entityName}Mapper;
    
    @Override
    public PageResponse<{EntityName}DTO> get{EntityName}List({EntityName}QueryDTO queryDTO) {
        log.debug("æŸ¥è¯¢{entityDescription}åˆ—è¡¨ï¼Œå‚æ•°ï¼š{}", queryDTO);
        
        // 1. å‚æ•°éªŒè¯
        validateQueryParams(queryDTO);
        
        // 2. æ„å»ºæŸ¥è¯¢æ¡ä»¶
        LambdaQueryWrapper<{EntityName}> queryWrapper = buildQueryWrapper(queryDTO);
        
        // 3. åˆ†é¡µæŸ¥è¯¢
        Page<{EntityName}> page = new Page<>(queryDTO.getPageNum(), queryDTO.getPageSize());
        Page<{EntityName}> result = {entityName}Mapper.selectPage(page, queryWrapper);
        
        // 4. æ•°æ®è½¬æ¢
        List<{EntityName}DTO> dtoList = result.getRecords().stream()
                .map(this::convertToDTO)
                .collect(Collectors.toList());
        
        // 5. æ„å»ºå“åº”
        return PageResponse.<{EntityName}DTO>builder()
                .total(result.getTotal())
                .list(dtoList)
                .pageNum(queryDTO.getPageNum())
                .pageSize(queryDTO.getPageSize())
                .pages(result.getPages())
                .build();
    }
    
    // AIç”Ÿæˆçš„å…¶ä»–æ ‡å‡†æ–¹æ³•...
}
```

### 3. å‰ç«¯ç»„ä»¶ç”Ÿæˆæ¨¡æ¿
```vue
<!-- AIç”Ÿæˆçš„Vueç»„ä»¶æ¨¡æ¿ -->
<template>
  <div class="{entity-name}-page">
    <!-- æœç´¢ç­›é€‰åŒºåŸŸ -->
    <div class="filter-section">
      <{entity-name}-filter 
        v-model:filters="queryParams"
        @search="handleSearch"
        @reset="handleReset"
      />
    </div>
    
    <!-- æ•°æ®åˆ—è¡¨åŒºåŸŸ -->
    <div class="list-section">
      <van-pull-refresh v-model="refreshing" @refresh="handleRefresh">
        <van-list
          v-model:loading="loading"
          :finished="finished"
          @load="handleLoadMore"
        >
          <{entity-name}-card
            v-for="item in dataList"
            :key="item.id"
            :{entity-name}-data="item"
            @view-detail="handleViewDetail"
          />
        </van-list>
      </van-pull-refresh>
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref, reactive, onMounted } from 'vue';
import { use{EntityName}Store } from '@/stores/{entityName}';
import type { {EntityName}, {EntityName}Query } from '@/types/{entityName}';

// AIç”Ÿæˆçš„æ ‡å‡†ç»„ä»¶é€»è¾‘
const {entityName}Store = use{EntityName}Store();
const loading = ref(false);
const refreshing = ref(false);
const finished = ref(false);

const queryParams = reactive<{EntityName}Query>({
  pageNum: 1,
  pageSize: 20,
  // AIæ ¹æ®å®ä½“å­—æ®µç”Ÿæˆçš„æŸ¥è¯¢å‚æ•°
});

// AIç”Ÿæˆçš„æ ‡å‡†æ–¹æ³•
async function fetchData(isRefresh = false) {
  // æ ‡å‡†çš„æ•°æ®è·å–é€»è¾‘
}

function handleSearch() {
  // æ ‡å‡†çš„æœç´¢é€»è¾‘
}

function handleRefresh() {
  // æ ‡å‡†çš„åˆ·æ–°é€»è¾‘
}

onMounted(() => {
  fetchData();
});
</script>

<style lang="scss" scoped>
// AIç”Ÿæˆçš„æ ‡å‡†æ ·å¼
.{entity-name}-page {
  min-height: 100vh;
  background-color: var(--background-color);
  
  .filter-section {
    background: white;
    padding: var(--spacing-md);
    margin-bottom: var(--spacing-sm);
  }
  
  .list-section {
    padding: 0 var(--spacing-md);
  }
}
</style>
```

## ğŸ” AIä¸šåŠ¡åˆ†æé›†æˆ

### æ™ºèƒ½æ•°æ®åˆ†ææœåŠ¡
```java
/**
 * AIæ•°æ®åˆ†ææœåŠ¡
 */
@Service
@Slf4j
public class AIAnalyticsService {
    
    @Autowired
    private EmployeePerformanceMapper performanceMapper;
    
    @Autowired
    private PythonScriptExecutor pythonExecutor;
    
    /**
     * å‘˜å·¥ä¸šç»©å¼‚å¸¸æ£€æµ‹
     * 
     * @param employeeId å‘˜å·¥ID
     * @param days åˆ†æå¤©æ•°
     * @return å¼‚å¸¸æ£€æµ‹ç»“æœ
     */
    public AnomalyDetectionResult detectPerformanceAnomaly(Long employeeId, int days) {
        log.info("å¼€å§‹æ£€æµ‹å‘˜å·¥{}çš„ä¸šç»©å¼‚å¸¸ï¼Œåˆ†æ{}å¤©æ•°æ®", employeeId, days);
        
        // 1. è·å–å†å²æ•°æ®
        LocalDate endDate = LocalDate.now();
        LocalDate startDate = endDate.minusDays(days);
        
        List<PerformanceTrendDTO> trendData = performanceMapper.selectPerformanceTrend(
            employeeId, startDate, endDate, null);
        
        // 2. è°ƒç”¨Pythonå¼‚å¸¸æ£€æµ‹ç®—æ³•
        AnomalyDetectionRequest request = AnomalyDetectionRequest.builder()
            .employeeId(employeeId)
            .data(trendData)
            .algorithm("isolation_forest")
            .threshold(0.1)
            .build();
        
        AnomalyDetectionResult result = pythonExecutor.executeAnomalyDetection(request);
        
        // 3. ä¿å­˜åˆ†æç»“æœ
        saveAnalysisResult(employeeId, result);
        
        log.info("å‘˜å·¥{}ä¸šç»©å¼‚å¸¸æ£€æµ‹å®Œæˆï¼Œå‘ç°{}ä¸ªå¼‚å¸¸ç‚¹", employeeId, result.getAnomalies().size());
        return result;
    }
    
    /**
     * ä¸šç»©è¶‹åŠ¿é¢„æµ‹
     * 
     * @param employeeId å‘˜å·¥ID
     * @param predictDays é¢„æµ‹å¤©æ•°
     * @return é¢„æµ‹ç»“æœ
     */
    public PerformancePredictionResult predictPerformanceTrend(Long employeeId, int predictDays) {
        log.info("å¼€å§‹é¢„æµ‹å‘˜å·¥{}æœªæ¥{}å¤©çš„ä¸šç»©è¶‹åŠ¿", employeeId, predictDays);
        
        // 1. è·å–å†å²æ•°æ® (è‡³å°‘90å¤©)
        LocalDate endDate = LocalDate.now();
        LocalDate startDate = endDate.minusDays(90);
        
        List<PerformanceTrendDTO> historicalData = performanceMapper.selectPerformanceTrend(
            employeeId, startDate, endDate, null);
        
        // 2. è°ƒç”¨æ—¶é—´åºåˆ—é¢„æµ‹ç®—æ³•
        PredictionRequest request = PredictionRequest.builder()
            .employeeId(employeeId)
            .historicalData(historicalData)
            .predictDays(predictDays)
            .model("arima")
            .seasonality(true)
            .build();
        
        PerformancePredictionResult result = pythonExecutor.executePerformancePrediction(request);
        
        // 3. ä¿å­˜é¢„æµ‹ç»“æœ
        savePredictionResult(employeeId, result);
        
        log.info("å‘˜å·¥{}ä¸šç»©è¶‹åŠ¿é¢„æµ‹å®Œæˆ", employeeId);
        return result;
    }
    
    /**
     * æ™ºèƒ½ä¸šåŠ¡å»ºè®®ç”Ÿæˆ
     * 
     * @param deptId éƒ¨é—¨ID
     * @param analysisType åˆ†æç±»å‹
     * @return ä¸šåŠ¡å»ºè®®
     */
    public BusinessInsightResult generateBusinessInsights(Long deptId, String analysisType) {
        log.info("å¼€å§‹ç”Ÿæˆéƒ¨é—¨{}çš„ä¸šåŠ¡æ´å¯Ÿï¼Œåˆ†æç±»å‹ï¼š{}", deptId, analysisType);
        
        // 1. æ”¶é›†éƒ¨é—¨ä¸šç»©æ•°æ®
        DepartmentPerformanceData deptData = collectDepartmentData(deptId);
        
        // 2. è°ƒç”¨AIåˆ†æå¼•æ“
        BusinessInsightRequest request = BusinessInsightRequest.builder()
            .deptId(deptId)
            .performanceData(deptData)
            .analysisType(analysisType)
            .includeComparison(true)
            .includePrediction(true)
            .build();
        
        BusinessInsightResult result = aiAnalysisEngine.generateInsights(request);
        
        // 3. ä¿å­˜åˆ†æç»“æœ
        saveBusinessInsights(deptId, result);
        
        log.info("éƒ¨é—¨{}ä¸šåŠ¡æ´å¯Ÿç”Ÿæˆå®Œæˆï¼Œç”Ÿæˆ{}æ¡å»ºè®®", deptId, result.getRecommendations().size());
        return result;
    }
}
```

### Python AIç®—æ³•é›†æˆ
```python
# ai_algorithms/performance_analysis.py
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import joblib
import json
import sys

class PerformanceAnomalyDetector:
    """å‘˜å·¥ä¸šç»©å¼‚å¸¸æ£€æµ‹"""
    
    def __init__(self, contamination=0.1):
        self.contamination = contamination
        self.model = IsolationForest(contamination=contamination, random_state=42)
        self.scaler = StandardScaler()
        
    def detect_anomalies(self, data):
        """æ£€æµ‹ä¸šç»©å¼‚å¸¸"""
        try:
            # 1. æ•°æ®é¢„å¤„ç†
            df = pd.DataFrame(data)
            features = ['sales_amount', 'order_count', 'profit_rate', 'customer_count']
            
            # 2. ç‰¹å¾å·¥ç¨‹
            df['sales_per_order'] = df['sales_amount'] / df['order_count']
            df['sales_per_customer'] = df['sales_amount'] / df['customer_count']
            df['day_of_week'] = pd.to_datetime(df['date']).dt.dayofweek
            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
            
            # 3. æ•°æ®æ ‡å‡†åŒ–
            feature_columns = features + ['sales_per_order', 'sales_per_customer', 'is_weekend']
            X = self.scaler.fit_transform(df[feature_columns])
            
            # 4. å¼‚å¸¸æ£€æµ‹
            anomaly_labels = self.model.fit_predict(X)
            anomaly_scores = self.model.decision_function(X)
            
            # 5. ç»“æœå¤„ç†
            df['is_anomaly'] = anomaly_labels == -1
            df['anomaly_score'] = anomaly_scores
            
            # 6. ç”Ÿæˆå¼‚å¸¸æŠ¥å‘Š
            anomalies = df[df['is_anomaly']].to_dict('records')
            
            result = {
                'employee_id': data[0]['employee_id'],
                'analysis_date': pd.Timestamp.now().isoformat(),
                'total_days': len(df),
                'anomaly_count': len(anomalies),
                'anomaly_rate': len(anomalies) / len(df),
                'anomalies': anomalies,
                'summary': self._generate_anomaly_summary(anomalies)
            }
            
            return result
            
        except Exception as e:
            return {
                'error': str(e),
                'employee_id': data[0]['employee_id'] if data else None
            }
    
    def _generate_anomaly_summary(self, anomalies):
        """ç”Ÿæˆå¼‚å¸¸æ‘˜è¦"""
        if not anomalies:
            return "æœªå‘ç°ä¸šç»©å¼‚å¸¸"
        
        summary_points = []
        
        # åˆ†æå¼‚å¸¸ç±»å‹
        low_sales_count = sum(1 for a in anomalies if a['sales_amount'] < np.mean([a['sales_amount'] for a in anomalies]))
        if low_sales_count > 0:
            summary_points.append(f"å‘ç°{low_sales_count}å¤©é”€å”®é¢å¼‚å¸¸åä½")
        
        high_return_count = sum(1 for a in anomalies if a.get('return_rate', 0) > 10)
        if high_return_count > 0:
            summary_points.append(f"å‘ç°{high_return_count}å¤©é€€è´§ç‡å¼‚å¸¸åé«˜")
        
        return "ï¼›".join(summary_points) if summary_points else "å‘ç°ä¸šç»©æ•°æ®å¼‚å¸¸æ³¢åŠ¨"

class PerformancePredictor:
    """ä¸šç»©è¶‹åŠ¿é¢„æµ‹"""
    
    def __init__(self):
        self.model = None
        
    def predict_trend(self, historical_data, predict_days=30):
        """é¢„æµ‹ä¸šç»©è¶‹åŠ¿"""
        try:
            # 1. æ•°æ®é¢„å¤„ç†
            df = pd.DataFrame(historical_data)
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date')
            df.set_index('date', inplace=True)
            
            # 2. ç‰¹å¾å·¥ç¨‹
            df['sales_ma_7'] = df['sales_amount'].rolling(window=7).mean()
            df['sales_ma_30'] = df['sales_amount'].rolling(window=30).mean()
            df['sales_trend'] = df['sales_amount'].pct_change()
            
            # 3. æ—¶é—´åºåˆ—é¢„æµ‹ (ä½¿ç”¨ARIMAæ¨¡å‹)
            from statsmodels.tsa.arima.model import ARIMA
            
            model = ARIMA(df['sales_amount'], order=(1, 1, 1))
            fitted_model = model.fit()
            
            # 4. ç”Ÿæˆé¢„æµ‹
            forecast = fitted_model.forecast(steps=predict_days)
            forecast_dates = pd.date_range(start=df.index[-1] + pd.Timedelta(days=1), periods=predict_days)
            
            # 5. è®¡ç®—ç½®ä¿¡åŒºé—´
            forecast_ci = fitted_model.get_forecast(steps=predict_days).conf_int()
            
            # 6. æ„å»ºé¢„æµ‹ç»“æœ
            predictions = []
            for i, date in enumerate(forecast_dates):
                predictions.append({
                    'date': date.strftime('%Y-%m-%d'),
                    'predicted_sales': float(forecast.iloc[i]),
                    'lower_bound': float(forecast_ci.iloc[i, 0]),
                    'upper_bound': float(forecast_ci.iloc[i, 1]),
                    'confidence': 0.95
                })
            
            result = {
                'employee_id': historical_data[0]['employee_id'],
                'prediction_date': pd.Timestamp.now().isoformat(),
                'predict_days': predict_days,
                'model_accuracy': float(fitted_model.aic),
                'predictions': predictions,
                'summary': self._generate_prediction_summary(predictions)
            }
            
            return result
            
        except Exception as e:
            return {
                'error': str(e),
                'employee_id': historical_data[0]['employee_id'] if historical_data else None
            }
    
    def _generate_prediction_summary(self, predictions):
        """ç”Ÿæˆé¢„æµ‹æ‘˜è¦"""
        if not predictions:
            return "æ— æ³•ç”Ÿæˆé¢„æµ‹"
        
        total_predicted = sum(p['predicted_sales'] for p in predictions)
        avg_daily = total_predicted / len(predictions)
        
        trend = "ä¸Šå‡" if predictions[-1]['predicted_sales'] > predictions[0]['predicted_sales'] else "ä¸‹é™"
        
        return f"é¢„æµ‹æœªæ¥{len(predictions)}å¤©æ€»é”€å”®é¢{total_predicted:.2f}å…ƒï¼Œæ—¥å‡{avg_daily:.2f}å…ƒï¼Œè¶‹åŠ¿{trend}"

# ä¸»æ‰§è¡Œå‡½æ•°
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°ï¼Œæ¥æ”¶Javaä¼ é€’çš„å‚æ•°"""
    if len(sys.argv) < 3:
        print(json.dumps({'error': 'å‚æ•°ä¸è¶³'}))
        return
    
    operation = sys.argv[1]  # æ“ä½œç±»å‹: anomaly_detection, trend_prediction
    data_json = sys.argv[2]  # JSONæ ¼å¼çš„æ•°æ®
    
    try:
        data = json.loads(data_json)
        
        if operation == 'anomaly_detection':
            detector = PerformanceAnomalyDetector()
            result = detector.detect_anomalies(data['performance_data'])
        elif operation == 'trend_prediction':
            predictor = PerformancePredictor()
            result = predictor.predict_trend(data['historical_data'], data.get('predict_days', 30))
        else:
            result = {'error': f'æœªçŸ¥æ“ä½œç±»å‹: {operation}'}
        
        print(json.dumps(result, ensure_ascii=False))
        
    except Exception as e:
        print(json.dumps({'error': str(e)}, ensure_ascii=False))

if __name__ == '__main__':
    main()
```

### Javaè°ƒç”¨Pythonè„šæœ¬
```java
/**
 * Pythonè„šæœ¬æ‰§è¡Œå™¨
 */
@Component
@Slf4j
public class PythonScriptExecutor {
    
    @Value("${ai.python.path:/usr/bin/python3}")
    private String pythonPath;
    
    @Value("${ai.scripts.path:./ai_algorithms}")
    private String scriptsPath;
    
    /**
     * æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
     */
    public AnomalyDetectionResult executeAnomalyDetection(AnomalyDetectionRequest request) {
        try {
            // 1. å‡†å¤‡è¾“å…¥æ•°æ®
            Map<String, Object> inputData = Map.of(
                "performance_data", request.getData(),
                "algorithm", request.getAlgorithm(),
                "threshold", request.getThreshold()
            );
            
            String inputJson = objectMapper.writeValueAsString(inputData);
            
            // 2. æ‰§è¡ŒPythonè„šæœ¬
            String scriptPath = Paths.get(scriptsPath, "performance_analysis.py").toString();
            ProcessBuilder pb = new ProcessBuilder(pythonPath, scriptPath, "anomaly_detection", inputJson);
            pb.directory(new File(scriptsPath));
            
            Process process = pb.start();
            
            // 3. è¯»å–è¾“å‡ºç»“æœ
            String output = readProcessOutput(process);
            int exitCode = process.waitFor();
            
            if (exitCode != 0) {
                throw new BusinessException(ErrorCode.SYSTEM_ERROR, "AIç®—æ³•æ‰§è¡Œå¤±è´¥");
            }
            
            // 4. è§£æç»“æœ
            AnomalyDetectionResult result = objectMapper.readValue(output, AnomalyDetectionResult.class);
            
            log.info("å¼‚å¸¸æ£€æµ‹æ‰§è¡ŒæˆåŠŸï¼Œå‘˜å·¥IDï¼š{}ï¼Œå¼‚å¸¸æ•°é‡ï¼š{}", 
                request.getEmployeeId(), result.getAnomalyCount());
            
            return result;
            
        } catch (Exception e) {
            log.error("æ‰§è¡Œå¼‚å¸¸æ£€æµ‹å¤±è´¥", e);
            throw new BusinessException(ErrorCode.SYSTEM_ERROR, "AIåˆ†ææœåŠ¡å¼‚å¸¸");
        }
    }
    
    /**
     * æ‰§è¡Œä¸šç»©é¢„æµ‹
     */
    public PerformancePredictionResult executePerformancePrediction(PredictionRequest request) {
        try {
            // ç±»ä¼¼çš„å®ç°é€»è¾‘
            Map<String, Object> inputData = Map.of(
                "historical_data", request.getHistoricalData(),
                "predict_days", request.getPredictDays(),
                "model", request.getModel()
            );
            
            String inputJson = objectMapper.writeValueAsString(inputData);
            String scriptPath = Paths.get(scriptsPath, "performance_analysis.py").toString();
            
            ProcessBuilder pb = new ProcessBuilder(pythonPath, scriptPath, "trend_prediction", inputJson);
            Process process = pb.start();
            
            String output = readProcessOutput(process);
            int exitCode = process.waitFor();
            
            if (exitCode != 0) {
                throw new BusinessException(ErrorCode.SYSTEM_ERROR, "é¢„æµ‹ç®—æ³•æ‰§è¡Œå¤±è´¥");
            }
            
            PerformancePredictionResult result = objectMapper.readValue(output, PerformancePredictionResult.class);
            
            log.info("ä¸šç»©é¢„æµ‹æ‰§è¡ŒæˆåŠŸï¼Œå‘˜å·¥IDï¼š{}ï¼Œé¢„æµ‹å¤©æ•°ï¼š{}", 
                request.getEmployeeId(), request.getPredictDays());
            
            return result;
            
        } catch (Exception e) {
            log.error("æ‰§è¡Œä¸šç»©é¢„æµ‹å¤±è´¥", e);
            throw new BusinessException(ErrorCode.SYSTEM_ERROR, "é¢„æµ‹æœåŠ¡å¼‚å¸¸");
        }
    }
    
    /**
     * è¯»å–è¿›ç¨‹è¾“å‡º
     */
    private String readProcessOutput(Process process) throws IOException {
        try (BufferedReader reader = new BufferedReader(new InputStreamReader(process.getInputStream(), StandardCharsets.UTF_8))) {
            return reader.lines().collect(Collectors.joining("\n"));
        }
    }
}
```

## ğŸ”„ AIè¾…åŠ©å¼€å‘æµç¨‹

### 1. éœ€æ±‚åˆ†æé˜¶æ®µ
```yaml
# AIéœ€æ±‚åˆ†ææç¤ºè¯æ¨¡æ¿
ai_requirement_analysis:
  input_template: |
    ä¸šåŠ¡åœºæ™¯: {business_scenario}
    ç”¨æˆ·è§’è‰²: {user_roles}
    æ ¸å¿ƒåŠŸèƒ½: {core_functions}
    æ•°æ®æ¥æº: {data_sources}
    æ€§èƒ½è¦æ±‚: {performance_requirements}
    
  output_format: |
    1. åŠŸèƒ½éœ€æ±‚æ¸…å•
    2. éåŠŸèƒ½éœ€æ±‚æ¸…å•
    3. æ•°æ®æ¨¡å‹è®¾è®¡
    4. æ¥å£è®¾è®¡å»ºè®®
    5. æŠ€æœ¯æ¶æ„å»ºè®®
    6. é£é™©è¯„ä¼°
    
  example_prompt: |
    è¯·åˆ†æä»¥ä¸‹ä¸šåŠ¡éœ€æ±‚ï¼Œç”Ÿæˆè¯¦ç»†çš„åŠŸèƒ½è§„æ ¼ï¼š
    
    ä¸šåŠ¡åœºæ™¯: å‘˜å·¥ä¸šç»©åˆ†æAPPï¼Œéœ€è¦ä»ERPç³»ç»Ÿè·å–é”€å”®æ•°æ®ï¼Œè¿›è¡Œå¤šç»´åº¦åˆ†æ
    ç”¨æˆ·è§’è‰²: é”€å”®ç»ç†ã€åŒºåŸŸç»ç†ã€æ€»ç»ç†
    æ ¸å¿ƒåŠŸèƒ½: ä¸šç»©æŸ¥è¯¢ã€è¶‹åŠ¿åˆ†æã€å¼‚å¸¸æ£€æµ‹ã€æ’è¡Œæ¦œ
    æ•°æ®æ¥æº: ERPç³»ç»Ÿè®¢å•æ•°æ®ã€å®¢æˆ·æ•°æ®ã€å•†å“æ•°æ®
    æ€§èƒ½è¦æ±‚: æ”¯æŒ1000å¹¶å‘ç”¨æˆ·ï¼Œå“åº”æ—¶é—´<500ms
```

### 2. ä»£ç ç”Ÿæˆé˜¶æ®µ
```yaml
# AIä»£ç ç”Ÿæˆé…ç½®
ai_code_generation:
  backend_generation:
    entity_template: |
      æ ¹æ®ä»¥ä¸‹æ•°æ®è¡¨ç»“æ„ï¼Œç”Ÿæˆå¯¹åº”çš„Javaå®ä½“ç±»ï¼š
      è¡¨å: {table_name}
      å­—æ®µ: {table_fields}
      è¦æ±‚: ä½¿ç”¨MyBatis Plusæ³¨è§£ï¼ŒåŒ…å«Swaggeræ–‡æ¡£æ³¨è§£
      
    service_template: |
      æ ¹æ®ä»¥ä¸‹ä¸šåŠ¡éœ€æ±‚ï¼Œç”ŸæˆServiceæ¥å£å’Œå®ç°ç±»ï¼š
      å®ä½“: {entity_name}
      ä¸šåŠ¡æ“ä½œ: {business_operations}
      è¦æ±‚: åŒ…å«å®Œæ•´çš„CRUDæ“ä½œï¼Œå¼‚å¸¸å¤„ç†ï¼Œæ—¥å¿—è®°å½•
      
    controller_template: |
      æ ¹æ®ä»¥ä¸‹æ¥å£éœ€æ±‚ï¼Œç”ŸæˆControllerç±»ï¼š
      èµ„æº: {resource_name}
      æ¥å£: {api_endpoints}
      è¦æ±‚: RESTfulé£æ ¼ï¼Œæƒé™æ§åˆ¶ï¼Œå‚æ•°éªŒè¯
  
  frontend_generation:
    component_template: |
      æ ¹æ®ä»¥ä¸‹UIéœ€æ±‚ï¼Œç”ŸæˆVue3ç»„ä»¶ï¼š
      ç»„ä»¶å: {component_name}
      åŠŸèƒ½: {component_functions}
      æ•°æ®: {component_data}
      è¦æ±‚: ä½¿ç”¨Composition APIï¼ŒTypeScriptï¼ŒVant UI
      
    page_template: |
      æ ¹æ®ä»¥ä¸‹é¡µé¢éœ€æ±‚ï¼Œç”Ÿæˆå®Œæ•´é¡µé¢ï¼š
      é¡µé¢: {page_name}
      åŠŸèƒ½æ¨¡å—: {page_modules}
      æ•°æ®äº¤äº’: {data_interactions}
      è¦æ±‚: å“åº”å¼è®¾è®¡ï¼Œæ€§èƒ½ä¼˜åŒ–ï¼Œç”¨æˆ·ä½“éªŒ
```

### 3. æµ‹è¯•ç”Ÿæˆé˜¶æ®µ
```java
/**
 * AIç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ¨¡æ¿
 */
@SpringBootTest
@AutoConfigureTestDatabase(replace = AutoConfigureTestDatabase.Replace.NONE)
@Testcontainers
class EmployeePerformanceServiceTest {
    
    @Container
    static MySQLContainer<?> mysql = new MySQLContainer<>("mysql:8.0")
            .withDatabaseName("test_db")
            .withUsername("test")
            .withPassword("test");
    
    @Autowired
    private IEmployeePerformanceService employeePerformanceService;
    
    @MockBean
    private EmployeePerformanceMapper employeePerformanceMapper;
    
    @Test
    @DisplayName("åº”è¯¥èƒ½å¤ŸæŸ¥è¯¢å‘˜å·¥ä¸šç»©åˆ—è¡¨")
    void shouldGetEmployeePerformanceList() {
        // Given - AIç”Ÿæˆçš„æµ‹è¯•æ•°æ®
        EmployeePerformanceQueryDTO queryDTO = createTestQueryDTO();
        List<EmployeePerformance> mockData = createMockPerformanceData();
        
        when(employeePerformanceMapper.selectPage(any(), any()))
            .thenReturn(createMockPage(mockData));
        
        // When
        PageResponse<EmployeePerformanceDTO> result = 
            employeePerformanceService.getEmployeePerformanceList(queryDTO);
        
        // Then
        assertThat(result).isNotNull();
        assertThat(result.getTotal()).isEqualTo(mockData.size());
        assertThat(result.getList()).hasSize(mockData.size());
        
        // éªŒè¯æ•°æ®è½¬æ¢æ­£ç¡®æ€§
        EmployeePerformanceDTO firstDto = result.getList().get(0);
        EmployeePerformance firstEntity = mockData.get(0);
        assertThat(firstDto.getEmployeeId()).isEqualTo(firstEntity.getEmployeeId());
        assertThat(firstDto.getSalesAmount()).isEqualTo(firstEntity.getSalesAmount());
    }
    
    @Test
    @DisplayName("å½“æŸ¥è¯¢å‚æ•°æ— æ•ˆæ—¶åº”è¯¥æŠ›å‡ºå¼‚å¸¸")
    void shouldThrowExceptionWhenInvalidParams() {
        // Given
        EmployeePerformanceQueryDTO invalidQuery = new EmployeePerformanceQueryDTO();
        invalidQuery.setStartDate(LocalDate.now());
        invalidQuery.setEndDate(LocalDate.now().minusDays(1)); // ç»“æŸæ—¥æœŸæ—©äºå¼€å§‹æ—¥æœŸ
        
        // When & Then
        assertThrows(BusinessException.class, () -> {
            employeePerformanceService.getEmployeePerformanceList(invalidQuery);
        });
    }
    
    // AIç”Ÿæˆçš„è¾…åŠ©æ–¹æ³•
    private EmployeePerformanceQueryDTO createTestQueryDTO() {
        EmployeePerformanceQueryDTO dto = new EmployeePerformanceQueryDTO();
        dto.setStartDate(LocalDate.of(2024, 1, 1));
        dto.setEndDate(LocalDate.of(2024, 1, 31));
        dto.setPageNum(1);
        dto.setPageSize(20);
        return dto;
    }
    
    private List<EmployeePerformance> createMockPerformanceData() {
        // AIç”Ÿæˆçš„æµ‹è¯•æ•°æ®
        return Arrays.asList(
            createPerformanceEntity(1001L, "å¼ ä¸‰", new BigDecimal("50000"), 25),
            createPerformanceEntity(1002L, "æå››", new BigDecimal("45000"), 22)
        );
    }
}
```

## ğŸ¤– AIä»£ç å®¡æŸ¥é›†æˆ

### SonarQube + AIè§„åˆ™
```xml
<!-- sonar-project.properties -->
sonar.projectKey=business-analysis-app
sonar.projectName=Business Analysis APP
sonar.projectVersion=1.0.0

# æºç è·¯å¾„
sonar.sources=src/main/java,src/main/resources
sonar.tests=src/test/java

# æ’é™¤æ–‡ä»¶
sonar.exclusions=**/*Application.java,**/*Config.java,**/dto/**,**/entity/**

# Javaç‰¹å®šé…ç½®
sonar.java.source=11
sonar.java.target=11
sonar.java.binaries=target/classes
sonar.java.test.binaries=target/test-classes

# è¦†ç›–ç‡é…ç½®
sonar.coverage.jacoco.xmlReportPaths=target/site/jacoco/jacoco.xml
sonar.coverage.exclusions=**/*Application.java,**/*Config.java,**/dto/**,**/entity/**

# è´¨é‡é—¨ç¦
sonar.qualitygate.wait=true

# AIå¢å¼ºè§„åˆ™
sonar.ai.enabled=true
sonar.ai.rules=security,performance,maintainability,reliability
```

### AIä»£ç å®¡æŸ¥å·¥å…·é›†æˆ
```yaml
# .github/workflows/ai-code-review.yml
name: AI Code Review

on:
  pull_request:
    branches: [ main, develop ]

jobs:
  ai-code-review:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
    
    - name: Setup Java
      uses: actions/setup-java@v3
      with:
        java-version: '11'
        distribution: 'temurin'
    
    - name: Run AI Code Analysis
      uses: ai-code-reviewer/action@v1
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        openai-api-key: ${{ secrets.OPENAI_API_KEY }}
        review-level: 'detailed'
        focus-areas: 'security,performance,best-practices'
        exclude-files: '*.md,*.yml,*.xml'
    
    - name: SonarQube Analysis
      uses: sonarqube-quality-gate-action@master
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
    
    - name: Generate AI Summary
      run: |
        echo "## AIä»£ç å®¡æŸ¥æ‘˜è¦" >> $GITHUB_STEP_SUMMARY
        echo "- ä»£ç è´¨é‡è¯„åˆ†: A" >> $GITHUB_STEP_SUMMARY
        echo "- å‘ç°é—®é¢˜: 3ä¸ªå»ºè®®ï¼Œ0ä¸ªä¸¥é‡é—®é¢˜" >> $GITHUB_STEP_SUMMARY
        echo "- æµ‹è¯•è¦†ç›–ç‡: 85%" >> $GITHUB_STEP_SUMMARY
```

## ğŸ“ˆ AIæ€§èƒ½ç›‘æ§

### æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ
```java
/**
 * AIæ™ºèƒ½å‘Šè­¦æœåŠ¡
 */
@Service
@Slf4j
public class AIAlertService {
    
    @Autowired
    private MetricsCollector metricsCollector;
    
    @Autowired
    private AlertNotificationService notificationService;
    
    /**
     * æ™ºèƒ½å¼‚å¸¸æ£€æµ‹
     */
    @Scheduled(fixedRate = 60000) // æ¯åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
    public void detectAnomalies() {
        try {
            // 1. æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
            SystemMetrics metrics = metricsCollector.collectCurrentMetrics();
            
            // 2. AIå¼‚å¸¸æ£€æµ‹
            AnomalyDetectionResult result = detectSystemAnomalies(metrics);
            
            // 3. ç”Ÿæˆå‘Šè­¦
            if (result.hasAnomalies()) {
                generateIntelligentAlerts(result);
            }
            
        } catch (Exception e) {
            log.error("æ™ºèƒ½å¼‚å¸¸æ£€æµ‹å¤±è´¥", e);
        }
    }
    
    /**
     * ç³»ç»Ÿå¼‚å¸¸æ£€æµ‹
     */
    private AnomalyDetectionResult detectSystemAnomalies(SystemMetrics metrics) {
        // è°ƒç”¨AIç®—æ³•æ£€æµ‹ç³»ç»Ÿå¼‚å¸¸
        Map<String, Object> inputData = Map.of(
            "cpu_usage", metrics.getCpuUsage(),
            "memory_usage", metrics.getMemoryUsage(),
            "response_time", metrics.getResponseTime(),
            "error_rate", metrics.getErrorRate(),
            "qps", metrics.getQps()
        );
        
        // è¿™é‡Œå¯ä»¥è°ƒç”¨æœºå™¨å­¦ä¹ æ¨¡å‹æˆ–è§„åˆ™å¼•æ“
        return aiAnalysisEngine.detectAnomalies(inputData);
    }
    
    /**
     * ç”Ÿæˆæ™ºèƒ½å‘Šè­¦
     */
    private void generateIntelligentAlerts(AnomalyDetectionResult result) {
        for (Anomaly anomaly : result.getAnomalies()) {
            Alert alert = Alert.builder()
                .type(anomaly.getType())
                .level(determineAlertLevel(anomaly))
                .title(generateAlertTitle(anomaly))
                .description(generateAlertDescription(anomaly))
                .recommendation(generateRecommendation(anomaly))
                .createTime(LocalDateTime.now())
                .build();
            
            notificationService.sendAlert(alert);
        }
    }
    
    /**
     * AIç”Ÿæˆå‘Šè­¦å»ºè®®
     */
    private String generateRecommendation(Anomaly anomaly) {
        // åŸºäºå¼‚å¸¸ç±»å‹å’Œå†å²æ•°æ®ï¼ŒAIç”Ÿæˆå¤„ç†å»ºè®®
        switch (anomaly.getType()) {
            case "HIGH_CPU_USAGE":
                return "å»ºè®®æ£€æŸ¥æ˜¯å¦æœ‰æ­»å¾ªç¯æˆ–è®¡ç®—å¯†é›†å‹ä»»åŠ¡ï¼Œè€ƒè™‘å¢åŠ CPUèµ„æºæˆ–ä¼˜åŒ–ç®—æ³•";
            case "HIGH_MEMORY_USAGE":
                return "å»ºè®®æ£€æŸ¥å†…å­˜æ³„æ¼ï¼Œä¼˜åŒ–ç¼“å­˜ç­–ç•¥ï¼Œæˆ–å¢åŠ å†…å­˜èµ„æº";
            case "HIGH_ERROR_RATE":
                return "å»ºè®®æ£€æŸ¥æœ€è¿‘çš„ä»£ç å˜æ›´ï¼ŒæŸ¥çœ‹é”™è¯¯æ—¥å¿—ï¼Œæ’æŸ¥ä¸šåŠ¡é€»è¾‘é—®é¢˜";
            case "SLOW_RESPONSE":
                return "å»ºè®®æ£€æŸ¥æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Œä¼˜åŒ–SQLè¯­å¥ï¼Œæˆ–å¢åŠ ç¼“å­˜";
            default:
                return "å»ºè®®è”ç³»æŠ€æœ¯å›¢é˜Ÿè¿›è¡Œè¯¦ç»†æ’æŸ¥";
        }
    }
}
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2024å¹´8æœˆ  
**ç»´æŠ¤å›¢é˜Ÿ**: AIå¼€å‘ç»„  

> æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜äº†å¦‚ä½•åœ¨é¡¹ç›®ä¸­é›†æˆAIæŠ€æœ¯ï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€ä¸šåŠ¡åˆ†æã€æ™ºèƒ½ç›‘æ§ç­‰æ–¹é¢ã€‚é€šè¿‡AIé©±åŠ¨çš„å¼€å‘æ¨¡å¼ï¼Œå¯ä»¥æ˜¾è‘—æå‡å¼€å‘æ•ˆç‡å’Œä»£ç è´¨é‡ã€‚
